{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85e48db-ff95-4afe-8bd8-df01c9d43c3c",
   "metadata": {},
   "source": [
    "# Tokenizer Walkthrough\n",
    "Described below is a detailed walkthrough of how a tokenizer is \"trained\". This tokenizer makes use of the byte pair encoding algorithmn to encode information into tokens that are often seen togther in order to establish a common vocabulary.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8581974-0cf7-4f66-8dd2-a8f02ce17a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031fc2f-85fb-4545-a222-e35cbd550a9e",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we make use of the tiny shakespeare dataset which is a simple plain text file containing the entire collection of Shakespeare's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b749f0-c85b-4d56-a7ee-97d22c4964c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Shakespear dataset as plain text\n",
    "file = open(\"Desktop/strata/data/shake.txt\", \"r\")\n",
    "file_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb2d96-d526-4bbe-86cb-0b0ec037e372",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "For convience we have defined configuration parameters at the top of the file for ease of adjustment.\n",
    "\n",
    "Rationale behind these settings is desribed below.\n",
    "\n",
    "> `MIN_VOCAB_SIZE` is set to 256 so that all the single byte UTF-8 characters are provided as a default.\n",
    "\n",
    "> `MAX_VOCAB_SIZE` is set to 86400 and is more open to interpretation. Most production grade LLM systems today use a tokenizer in the 80K-100K range. This seems to be just a result of experimentation, but more research is required in order to justify and possibly refine this decision.\n",
    "\n",
    "> `REGEX_PATTERN` is set to a complicated set of conditions to match over. Some of the major points are:\n",
    "> - Separating out the stem of words from their contraction component, for example `don't` becomes `don` and `'t`. This way the LLM model will develop an understanding of root words and the collection of possible contractions.\n",
    "> - Any numbers larger than 3 digits is separated out into their own token, for example `100000` becomes `100` and `000`. This has many advantages, most notablly allowing the LLM to start understanding the common convention of using commas to separate 3 digits groupings, but also to prevent the tokenizer training from merging long strings of the same digit into a single token which could lead the model astray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42219b41-1334-4adb-ad1c-c9dde6a8ddee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MIN_VOCAB_SIZE = 256\n",
    "MAX_VOCAB_SIZE = 86400\n",
    "REGEX_PATTERN = regex.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]\n",
    "    ++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c484c07-7d65-40d1-9492-6bc536b0a4a7",
   "metadata": {},
   "source": [
    "## Number of Merges\n",
    "The number of merges is defined as the difference between the the min and max vocab size. This basically indicates how many new tokens need to be created in order to get to the established vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe3d68e-830d-43a5-8945-195f8a0e1990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Merges: 86144\n"
     ]
    }
   ],
   "source": [
    "# Calculate the possible number of merges\n",
    "num_merges = MAX_VOCAB_SIZE - MIN_VOCAB_SIZE\n",
    "print(f\"Number of Merges: {num_merges}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085afcd1-c214-4798-adbf-7119ae3b6ffd",
   "metadata": {},
   "source": [
    "## Find All Regex Patterns\n",
    "\n",
    "Here we use the regex pattern defined above to split our training text into a list of text chunks where each chunk successfully matches a pattern defined in the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e378396e-7d4b-4bef-9c27-35e3f8215a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Text Chunks: 2\n",
      "Example Text Chunks: ['Hello', ' World']\n"
     ]
    }
   ],
   "source": [
    "# Get all matching regex patterns\n",
    "text_chunks = regex.findall(REGEX_PATTERN, \"Hello World!\")\n",
    "print(f\"Number of Text Chunks: {len(text_chunks)}\")\n",
    "print(f\"Example Text Chunks: {text_chunks[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ba9e0-4c1f-455a-9e7b-4a9a38af2af4",
   "metadata": {},
   "source": [
    "# Conversion to Byte Identifers\n",
    "Here we take each chunk and encode it using the UTF-8 standard, for example `\"abc\".encode(\"utf-8\"))` produces `b'abc'`. If we convert these Python byte representations to a list, it will get coersed to integers corresponding to the character value, for example the previous value produces `[97, 98, 99]` for a, b, and c. The final output `token_ids` is a list of lists where each inner list is a chunk represented as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "aef5e47f-321a-4f48-a67d-b7fe718ad86c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Byte Representation: [[72, 101, 108, 108, 111], [32, 87, 111, 114, 108, 100]]\n"
     ]
    }
   ],
   "source": [
    "byte_int_maps = [string_byte_int_map(chunk) for chunk in text_chunks]\n",
    "print(f\"Integer Byte Representation: {byte_int_maps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f302cb-5ada-471d-be88-ea20ad92df5c",
   "metadata": {},
   "source": [
    "## Initialize Vocabulary\n",
    "We initialize the vocabulary by seting up the tokens for the initial vocab size. As mentioned in the configuration this is set to the first 256 integers so that all single byte UTF-8 characters are supported by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "98514beb-d615-4582-8e57-3f783d4f3841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179, 180: 180, 181: 181, 182: 182, 183: 183, 184: 184, 185: 185, 186: 186, 187: 187, 188: 188, 189: 189, 190: 190, 191: 191, 192: 192, 193: 193, 194: 194, 195: 195, 196: 196, 197: 197, 198: 198, 199: 199, 200: 200, 201: 201, 202: 202, 203: 203, 204: 204, 205: 205, 206: 206, 207: 207, 208: 208, 209: 209, 210: 210, 211: 211, 212: 212, 213: 213, 214: 214, 215: 215, 216: 216, 217: 217, 218: 218, 219: 219, 220: 220, 221: 221, 222: 222, 223: 223, 224: 224, 225: 225, 226: 226, 227: 227, 228: 228, 229: 229, 230: 230, 231: 231, 232: 232, 233: 233, 234: 234, 235: 235, 236: 236, 237: 237, 238: 238, 239: 239, 240: 240, 241: 241, 242: 242, 243: 243, 244: 244, 245: 245, 246: 246, 247: 247, 248: 248, 249: 249, 250: 250, 251: 251, 252: 252, 253: 253, 254: 254, 255: 255}\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: idx for idx in range(MIN_VOCAB_SIZE)}\n",
    "print(f\"Initial Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196459a2-7e65-421f-a563-61cf67a76c8f",
   "metadata": {},
   "source": [
    "## Get the Number of Conseciti\n",
    "This function takes an array of integers and looks at all the pairs in which it constructs a frequency table of how many times each occurs. For example, given an input `[1, 2, 3, 1, 2]` it returns\n",
    "```python\n",
    "{\n",
    "   (1, 2): 2, \n",
    "   (2, 3): 1, \n",
    "   (3, 1): 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8fead77b-23db-4fae-ad56-27c33a8a287a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_num_pairs(ids: [int]) -> {(int, int), int}:\n",
    "    \"\"\"\n",
    "    Takes an array of integers, groups them into consecutive pairs and records the number of each.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    Input: ids = [108, 101, 108, 101]\n",
    "    Output: {\n",
    "                (108, 101): 2, \n",
    "                (101, 108): 1\n",
    "            }\n",
    "    \"\"\"\n",
    "    table = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        table[pair] = table.get(pair, 0) + 1\n",
    "    return table\n",
    "\n",
    "def char_byte_int_map(char: str) -> int:\n",
    "    \"\"\"\n",
    "    Converts a single character into its integer byte representation\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    Input: char = \"a\"\n",
    "    Output: 97\n",
    "    \"\"\"\n",
    "    return list(char.encode(\"utf-8\"))[0]\n",
    "\n",
    "def string_byte_int_map(string: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Converts a string (sequence of characters) into an array of its respective integer byte representations.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    Input: string = \"hello\"\n",
    "    Output: [104, 101, 108, 108, 111]\n",
    "    \"\"\"\n",
    "    return [byte_int_map(char) for char in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "394cb989-f451-4b1c-840b-507cf867d5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bytes([23]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af3cfc-dc6b-4769-b20e-b173b8451750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
